# Representing-Sentences-with-Neural-Models
Train neural network models to obtain sentence representations. We then used these sentence representations for a downstream task such as sentiment classification.

Project as part of the NLP 1 modules at UvA.

In our project, we developed neural network models to extract sentence representations and applied these for sentiment classification. Collaboratively, we documented our experimental procedures and findings in a detailed scientific report, ensuring all methods were fully reproducible.

Using the Stanford Sentiment Treebank (SST), we explored different modeling techniques including Bag-of-words (BOW), Continuous Bag-of-Words (CBOW), Deep CBOW, LSTM, and Tree-LSTM. Our primary objective was to assess the impact of word order and tree structures on sentiment analysis accuracy.

We began by evaluating simpler models like BOW and CBOW, which disregard word order, and progressed to more complex models like LSTM and Tree-LSTM that consider syntactic structures. Our findings indicated a significant advantage in models that preserved word order and utilized tree structures, reflecting their superior capability in capturing the nuances of sentiment.

The completion of this project not only advanced our understanding of neural networks in NLP but also reinforced the importance of thorough documentation and reproducibility in scientific experiments. Our combined efforts resulted in a comprehensive analysis, providing valuable insights into the effectiveness of various sentence representation models in sentiment classification tasks.

### This Repo

See pdf of the assignment write up for details about this project, our experiments and results.
See the python notebook for the implementation, note most of the code was given by the course creators and the parts done by myself and my lab partner are clearly defined.






